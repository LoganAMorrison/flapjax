{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import pathlib\n",
    "\n",
    "# Environment\n",
    "import gym\n",
    "\n",
    "# Jax, Flax, Optax\n",
    "import jax\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import checkpoints\n",
    "import optax\n",
    "\n",
    "# PPO\n",
    "import proximal_policy_optimization as ppo\n",
    "\n",
    "# other\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is need on my machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return gym.make(\"CartPole-v1\")\n",
    "\n",
    "\n",
    "def make_vector_env(num_envs: int = 8, asynchronous: bool = False):\n",
    "    if asynchronous:\n",
    "        env = gym.vector.AsyncVectorEnv([make_env for _ in range(num_envs)])  # type: ignore\n",
    "    else:\n",
    "        env = gym.vector.SyncVectorEnv([make_env for _ in range(num_envs)])  # type: ignore\n",
    "    return env\n",
    "\n",
    "\n",
    "train_env = make_vector_env()\n",
    "eval_env = make_env()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_num_opt_steps(\n",
    "    total_frames: int, horizon: int, n_actors: int, epochs: int, mini_batch_size: int\n",
    ") -> int:\n",
    "    \"\"\"Compute the number of optimization steps.\"\"\"\n",
    "    batch_size = horizon * n_actors\n",
    "    # Number of frames we see per train step\n",
    "    frames_per_train_step = batch_size\n",
    "    # Number of times we call optimizer per step\n",
    "    opt_steps_per_train_step = epochs * (batch_size // mini_batch_size)\n",
    "    # Number of train steps\n",
    "    num_train_steps = total_frames // frames_per_train_step\n",
    "    # Total number of optimizer calls\n",
    "    total_opt_steps = opt_steps_per_train_step * num_train_steps\n",
    "\n",
    "    return total_opt_steps\n",
    "\n",
    "\n",
    "total_frames = int(1e5)\n",
    "n_actors = 8\n",
    "horizon = 32\n",
    "mini_batch_size = 256\n",
    "epochs = 20\n",
    "total_opt_steps = ppo_num_opt_steps(\n",
    "    total_frames, horizon, n_actors, epochs, mini_batch_size\n",
    ")\n",
    "\n",
    "epsilon = optax.linear_schedule(0.1, 0.0, total_opt_steps)\n",
    "learning_rate = optax.linear_schedule(2.5e-4, 0.0, total_opt_steps)\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "# Configuration\n",
    "config = ppo.PPOConfig(\n",
    "    n_actors=n_actors,\n",
    "    total_frames=total_frames,\n",
    "    horizon=horizon,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    lam=0.8,\n",
    "    gamma=0.98,\n",
    "    epochs=epochs,\n",
    "    c1=1.0,\n",
    "    c2=0.0,\n",
    "    epsilon=epsilon,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "key = jax.random.PRNGKey(0)\n",
    "n_hidden = 512\n",
    "n_actions = train_env.action_space[0].n  # type: ignore\n",
    "model = ppo.ActorCriticMlp(n_hidden=n_hidden, n_actions=n_actions)\n",
    "\n",
    "# Initialize model\n",
    "observation = ppo.env_reset(train_env)\n",
    "key, rng = jax.random.split(key, 2)\n",
    "params = model.init(rng, observation)\n",
    "state = ppo.PPOTrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    lr=learning_rate,\n",
    "    config=config,\n",
    "    max_grad_norm=max_grad_norm,\n",
    ")\n",
    "del params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log and checkpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = (\n",
    "    pathlib.Path(\".\").absolute().joinpath(\"checkpoints/cartpole/run1\").as_posix()\n",
    ")\n",
    "log_dir = pathlib.Path(\".\").absolute().joinpath(\"logs/cartpole/run1\").as_posix()\n",
    "\n",
    "log_frequency = 1\n",
    "eval_frequency = 1\n",
    "eval_episodes = 100\n",
    "\n",
    "summary_writer = tensorboard.SummaryWriter(log_dir)\n",
    "summary_writer.hparams(config._asdict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config.horizon * config.n_actors\n",
    "frames_per_train_step = batch_size\n",
    "num_train_steps = config.total_frames // frames_per_train_step\n",
    "\n",
    "reward = 0.0\n",
    "\n",
    "horizon = state.config.horizon\n",
    "gamma = state.config.gamma\n",
    "lam = state.config.lam\n",
    "\n",
    "with tqdm(range(num_train_steps)) as t:\n",
    "    for step in t:\n",
    "        frame = step * frames_per_train_step\n",
    "        t.set_description(f\"frame: {step}\")\n",
    "\n",
    "        key, rng1, rng2 = jax.random.split(key, 3)\n",
    "        trajectory, observation = ppo.create_trajectory(\n",
    "            observation,\n",
    "            state.apply_fn,\n",
    "            state.params,\n",
    "            train_env,\n",
    "            rng1,\n",
    "            horizon,\n",
    "            gamma,\n",
    "            lam,\n",
    "        )\n",
    "        state, losses = ppo.train_step(state, trajectory, rng2)\n",
    "\n",
    "        if step % log_frequency == 0:\n",
    "            summary_writer.scalar(\"train/loss\", losses[\"total\"], frame)\n",
    "            summary_writer.scalar(\"train/loss-actor\", losses[\"actor\"], frame)\n",
    "            summary_writer.scalar(\"train/loss-critic\", losses[\"critic\"], frame)\n",
    "            summary_writer.scalar(\"train/loss-entropy\", losses[\"entropy\"], frame)\n",
    "            summary_writer.scalar(\"train/learning-rate\", state.learning_rate(), frame)\n",
    "            summary_writer.scalar(\"train/clipping\", state.epsilon(), frame)\n",
    "\n",
    "        if step % 25 == 0:\n",
    "            key, rng = jax.random.split(key, 2)\n",
    "            reward = ppo.evaluate_model(state, eval_env, eval_episodes, rng)\n",
    "            summary_writer.scalar(\"train/reward\", reward, frame)\n",
    "\n",
    "        t.set_description_str(f\"loss: {losses['total']}, reward: {reward}\")\n",
    "\n",
    "        if checkpoint_dir is not None:\n",
    "            checkpoints.save_checkpoint(checkpoint_dir, state, frame)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
