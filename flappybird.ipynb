{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import pathlib\n",
    "\n",
    "# Environment\n",
    "from flappy_bird import FlappyBirdEnvV0\n",
    "import gym\n",
    "from gym.wrappers import GrayScaleObservation, ResizeObservation\n",
    "from proximal_policy_optimization import FrameSkip\n",
    "\n",
    "# Jax, Flax, Optax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import checkpoints\n",
    "import optax\n",
    "\n",
    "# PPO\n",
    "import proximal_policy_optimization as ppo\n",
    "\n",
    "# other\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not really needed, but allows us to use 'gym.make'.\n",
    "gym.register(\n",
    "    id=\"FlappyBird-v0\",\n",
    "    entry_point=\"flappy_bird.envs:FlappyBirdEnvV0\",\n",
    ")\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"FlappyBird-v0\")\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    env = GrayScaleObservation(env, keep_dim=True)\n",
    "    env = FrameSkip(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def make_vector_env(num_envs: int = 8, asynchronous: bool = False):\n",
    "    if asynchronous:\n",
    "        env = gym.vector.AsyncVectorEnv([make_env for _ in range(num_envs)])  # type: ignore\n",
    "    else:\n",
    "        env = gym.vector.SyncVectorEnv([make_env for _ in range(num_envs)])  # type: ignore\n",
    "    return env\n",
    "\n",
    "\n",
    "train_env = make_vector_env()\n",
    "eval_env = make_env()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_num_opt_steps(\n",
    "    total_frames: int, horizon: int, n_actors: int, epochs: int, mini_batch_size: int\n",
    ") -> int:\n",
    "    \"\"\"Compute the number of optimization steps.\"\"\"\n",
    "    batch_size = horizon * n_actors\n",
    "    # Number of frames we see per train step\n",
    "    frames_per_train_step = batch_size\n",
    "    # Number of times we call optimizer per step\n",
    "    opt_steps_per_train_step = epochs * (batch_size // mini_batch_size)\n",
    "    # Number of train steps\n",
    "    num_train_steps = total_frames // frames_per_train_step\n",
    "    # Total number of optimizer calls\n",
    "    total_opt_steps = opt_steps_per_train_step * num_train_steps\n",
    "\n",
    "    return total_opt_steps\n",
    "\n",
    "# These parameters are from https://github.com/araffin/rl-baselines-zoo/blob/master/hyperparams/ppo2.yml \n",
    "total_frames = int(1e7)\n",
    "n_actors = 8\n",
    "horizon = 128\n",
    "mini_batch_size = 256\n",
    "epochs = 4\n",
    "total_opt_steps = ppo_num_opt_steps(\n",
    "    total_frames, horizon, n_actors, epochs, mini_batch_size\n",
    ")\n",
    "\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "epsilon = optax.linear_schedule(0.1, 0.0, total_opt_steps)\n",
    "c1 = 0.5\n",
    "c2 = 0.01\n",
    "learning_rate = optax.linear_schedule(2.5e-4, 0.0, total_opt_steps)\n",
    "max_grad_norm = 0.5\n",
    "clip_reward = False\n",
    "\n",
    "# Configuration\n",
    "config = ppo.PPOConfig(\n",
    "    n_actors=n_actors,\n",
    "    total_frames=total_frames,\n",
    "    horizon=horizon,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    lam=lam,\n",
    "    gamma=gamma,\n",
    "    epochs=epochs,\n",
    "    c1=c1,\n",
    "    c2=c2,\n",
    "    epsilon=epsilon,\n",
    "    clip_reward=clip_reward,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "key = jax.random.PRNGKey(0)\n",
    "n_hidden = 512\n",
    "n_actions = train_env.action_space[0].n  # type: ignore\n",
    "model = ppo.ActorCriticCnn(n_hidden=n_hidden, n_actions=n_actions)\n",
    "\n",
    "# Initialize model\n",
    "observation = ppo.env_reset(train_env)\n",
    "key, rng = jax.random.split(key, 2)\n",
    "params = model.init(rng, observation)\n",
    "state = ppo.PPOTrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    lr=learning_rate,\n",
    "    config=config,\n",
    "    max_grad_norm=max_grad_norm,\n",
    ")\n",
    "del params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure logging and checkpointing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = (\n",
    "    pathlib.Path(\".\").absolute().joinpath(\"checkpoints/flappy_bird/run1\").as_posix()\n",
    ")\n",
    "log_dir = pathlib.Path(\".\").absolute().joinpath(\"logs/flappy_bird/run1\").as_posix()\n",
    "\n",
    "summary_writer = tensorboard.SummaryWriter(log_dir)\n",
    "summary_writer.hparams(config._asdict())\n",
    "\n",
    "log_frequency = 1\n",
    "eval_frequency = 25\n",
    "eval_episodes = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8b0c09f0a3468a8a96c4195feedf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = config.horizon * config.n_actors\n",
    "frames_per_train_step = batch_size\n",
    "num_train_steps = config.total_frames // frames_per_train_step\n",
    "\n",
    "reward = 0.0\n",
    "\n",
    "horizon = state.config.horizon\n",
    "gamma = state.config.gamma\n",
    "lam = state.config.lam\n",
    "\n",
    "with tqdm(range(num_train_steps)) as t:\n",
    "    for step in t:\n",
    "        frame = step * frames_per_train_step\n",
    "        t.set_description(f\"frame: {step}\")\n",
    "\n",
    "        key, rng1, rng2 = jax.random.split(key, 3)\n",
    "        trajectory, observation = ppo.create_trajectory(\n",
    "            observation,\n",
    "            state.apply_fn,\n",
    "            state.params,\n",
    "            train_env,\n",
    "            rng1,\n",
    "            horizon,\n",
    "            gamma,\n",
    "            lam,\n",
    "        )\n",
    "        state, losses = ppo.train_step(state, trajectory, rng2)\n",
    "\n",
    "        if step % log_frequency == 0:\n",
    "            summary_writer.scalar(\"train/loss\", losses[\"total\"], frame)\n",
    "            summary_writer.scalar(\"train/loss-actor\", losses[\"actor\"], frame)\n",
    "            summary_writer.scalar(\"train/loss-critic\", losses[\"critic\"], frame)\n",
    "            summary_writer.scalar(\"train/loss-entropy\", losses[\"entropy\"], frame)\n",
    "            summary_writer.scalar(\"train/learning-rate\", state.learning_rate(), frame)\n",
    "            summary_writer.scalar(\"train/clipping\", state.epsilon(), frame)\n",
    "\n",
    "        if step % eval_frequency == 0:\n",
    "            key, rng = jax.random.split(key, 2)\n",
    "            reward = ppo.evaluate_model(state, eval_env, eval_episodes, rng)\n",
    "            summary_writer.scalar(\"train/reward\", reward, frame)\n",
    "\n",
    "        t.set_description_str(f\"loss: {losses['total']}, reward: {reward}\")\n",
    "\n",
    "        if checkpoint_dir is not None:\n",
    "            checkpoints.save_checkpoint(checkpoint_dir, state, frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
